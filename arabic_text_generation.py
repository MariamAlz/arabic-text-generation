# -*- coding: utf-8 -*-
"""Arabic Text Generation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yr8Qzq72GMW1TaCV4ZXXjnpkY7zMgxvi
"""

!pip install transformers 
!pip install farasapy 
!pip install pyarabic

!git clone https://github.com/aub-mind/arabert.git

#import GPT2LMHeadModel for Text generation and GPT2Tokenizer for tokenizing the text.
from transformers import GPT2TokenizerFast, pipeline
#for base and medium
from transformers import GPT2LMHeadModel
#for large and mega
from arabert.preprocess import ArabertPreprocessor

"""AraGPT-2 base"""

MODEL_NAME='aubmindlab/aragpt2-base'
arabert_prep = ArabertPreprocessor(model_name=MODEL_NAME)

model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)
tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_NAME)
generation_pipeline = pipeline("text-generation",model=model,tokenizer=tokenizer)

"""Text prompt"""

sentence = "شعرت بالسعادة"

input_ids = tokenizer.encode(sentence, return_tensors = 'pt')

input_ids

tokenizer.decode(input_ids[0])

def generate_text(text):
  text_clean = arabert_prep.preprocess(text)
  result = generation_pipeline(text_clean,
                                pad_token_id=tokenizer.eos_token_id,
                                num_beams=10,
                                max_length=100,
                                top_p=0.9,
                                repetition_penalty = 3.0,
                                no_repeat_ngram_size = 3)[0]['generated_text']
  return result

generate_text(sentence)

"""GPT2-Small-Arabic"""

!pip install -q transformers

from transformers import AutoTokenizer, AutoModelWithLMHead
tokenizer = AutoTokenizer.from_pretrained("akhooli/gpt2-small-arabic")
model = AutoModelWithLMHead.from_pretrained("akhooli/gpt2-small-arabic")

import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

sentence2 = "القدس مدينة تاريخية تقع في "

def generate_text(text):
  try:
    input_ids = tokenizer.encode(sentence2, return_tensors='pt').to(device) 
    model.to(device)
    max_length = 100
    #set top_k = 40 and num_return_sequences = 3
    sample_outputs = model.generate(input_ids, pad_token_id=50256,
                                      do_sample=True, 
                                      max_length=max_length, 
                                      min_length=max_length,
                                      top_k=40,
                                      num_return_sequences=5)

    for i, sample_output in enumerate(sample_outputs):
        print(">> Generated text {}\n\n{}".format(i+1, tokenizer.decode(sample_output.tolist())))
        print('\n---')
  except exception as e:
    print("Something Wrong",e)

generate_text(sentence2)

"""GPT2-Small-Arabic-Poetry"""

tokenizer= AutoTokenizer.from_pretrained("akhooli/gpt2-small-arabic-poetry")
model = AutoModelWithLMHead.from_pretrained("akhooli/gpt2-small-arabic-poetry")

sentence3 = "الحب"

def generate_text(text):
  try:
    input_ids = tokenizer.encode(sentence3, return_tensors='pt').to(device) 
    model.to(device)
    max_length = 100
    #set top_k = 40 and num_return_sequences = 3
    sample_outputs = model.generate(input_ids, pad_token_id=50256,
                                      do_sample=True, 
                                      max_length=max_length, 
                                      min_length=max_length,
                                      top_k=40,
                                      num_return_sequences=5)

    for i, sample_output in enumerate(sample_outputs):
        print(">> Generated text {}\n\n{}".format(i+1, tokenizer.decode(sample_output.tolist())))
        print('\n---')
  except exception as e:
    print("Something Wrong",e)

generate_text(sentence3)

"""Fine-tuning the based on @HHShkMohd tweets


"""

!pip install --upgrade simpletransformers tokenizers==0.9.4
!pip install pytorch cpuonly -c pytorch

! head -n 5 HHShkMohd_tweets.csv

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import string
import re
import nltk
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix,accuracy_score, classification_report

data = pd.read_csv(r"HHShkMohd_tweets.csv")
print(data.head())
print(data.sample(5))

punctuations = '''`÷×؛<>_()*&^%][ـ،/:"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation

nltk.download('stopwords')
stop_words = stopwords.words()

arabic_diacritics = re.compile("""
                             ّ    | # Shadda
                             َ    | # Fatha
                             ً    | # Tanwin Fath
                             ُ    | # Damma
                             ٌ    | # Tanwin Damm
                             ِ    | # Kasra
                             ٍ    | # Tanwin Kasr
                             ْ    | # Sukun
                             ـ     # Tatwil/Kashida
                         """, re.VERBOSE)

def preprocess(text):
    
    #remove punctuations
    translator = str.maketrans('', '', punctuations)
    text = text.translate(translator)
    
    #remove Tashkeel
    text = re.sub(arabic_diacritics, '', text)
    
    #remove longation
    text = re.sub("[إأآا]", "ا", text)
    text = re.sub("ى", "ي", text)
    text = re.sub("ؤ", "ء", text)
    text = re.sub("ئ", "ء", text)
    text = re.sub("ة", "ه", text)
    text = re.sub("گ", "ك", text)

    #remove URL
    text = re.sub(r"http\S+", "", text)

    #remove mentions and hashtags
    mentions = re.findall("@([a-zA-Z0-9_]{1,50})", text)
    hashtags = re.findall("#([a-zA-Z0-9_]{1,50})", text)
    text = re.sub("@[A-Za-z0-9_]+","", text)
    text = re.sub("#[A-Za-z0-9_]+","", text)

    text = ' '.join(word for word in text.split() if word not in stop_words)

    return text
    
    data['Feed'] = data['Feed'].apply(preprocess)

print(data.head(5))

import csv
import random

train = []
test = []
eval = []

users = {
    'HH', '[HHShkMohd] '
}

for users in [['HH', '[HHShkMohd] ']]:
  f = open(r"HHShkMohd_tweets.csv", "r")
   
  rdr = csv.reader(f, delimiter='\t')
  lines = 0
  for line in rdr:
      lines += 1
      if line == 1:
        continue
      if len(line) == 1:
        if random.random() > 0.4:
          train.append(arabert_prep.preprocess(line[0])  + '<|endoftext|>')
        elif random.random() > 0.2:
          test.append(arabert_prep.preprocess(line[0])  + '<|endoftext|>')
        else:
          eval.append(arabert_prep.preprocess(line[0])  + '<|endoftext|>')

train[-1]

open('./train.txt', 'w').write("\n".join(train))
open('./test.txt', 'w').write("\n".join(test))
open('./eval.txt', 'w').write("\n".join(test))
len(train)

from simpletransformers.language_modeling import LanguageModelingModel, LanguageModelingArgs

train_args = {
    "reprocess_input_data": True,
    "overwrite_output_dir": True,
    "train_batch_size": 8,
    "num_train_epochs": 10,
    "fp16": False,
    "mlm": False,
}

ft_model = LanguageModelingModel('gpt2', 'aubmindlab/aragpt2-base', args=train_args)

ft_model.tokenizer.add_tokens(["[HHShkMohd]"])
ft_model.model.resize_token_embeddings(len(ft_model.tokenizer))

ft_model.train_model("./train.txt", eval_file="./test.txt")

ft_model.eval_model("./eval.txt")

ft_model.tokenizer.save_pretrained("./users")

ft_model.model.save_pretrained("./users")

from simpletransformers.language_generation import LanguageGenerationModel
model = LanguageGenerationModel("gpt2", "./users")

prompt = "مدينة دبي"

model.tokenizer.encode('[HHShkMohd] ' + prompt)

model.generate('[HHShkMohd] ' + prompt)

model.generate('[HHShkMohd] ' + prompt, { 'max_length': 50 })