# -*- coding: utf-8 -*-
"""Arabic Text Generation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yr8Qzq72GMW1TaCV4ZXXjnpkY7zMgxvi
"""

!pip install transformers 
!pip install farasapy 
!pip install pyarabic

!git clone https://github.com/aub-mind/arabert.git

#import GPT2LMHeadModel for Text generation and GPT2Tokenizer for tokenizing the text.
from transformers import GPT2TokenizerFast, pipeline
#for base and medium
from transformers import GPT2LMHeadModel
#for large and mega
from arabert.preprocess import ArabertPreprocessor

"""AraGPT-2 base"""

MODEL_NAME='aubmindlab/aragpt2-base'
arabert_prep = ArabertPreprocessor(model_name=MODEL_NAME)

model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)
tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_NAME)
generation_pipeline = pipeline("text-generation",model=model,tokenizer=tokenizer)

"""Text prompt"""

sentence = "شعرت بالسعادة"

input_ids = tokenizer.encode(sentence, return_tensors = 'pt')

input_ids

tokenizer.decode(input_ids[0])

def generate_text(text):
  text_clean = arabert_prep.preprocess(text)
  result = generation_pipeline(text_clean,
                                pad_token_id=tokenizer.eos_token_id,
                                num_beams=10,
                                max_length=100,
                                top_p=0.9,
                                repetition_penalty = 3.0,
                                no_repeat_ngram_size = 3)[0]['generated_text']
  return result

generate_text(sentence)

"""GPT2-Small-Arabic"""

!pip install -q transformers

from transformers import AutoTokenizer, AutoModelWithLMHead
tokenizer = AutoTokenizer.from_pretrained("akhooli/gpt2-small-arabic")
model = AutoModelWithLMHead.from_pretrained("akhooli/gpt2-small-arabic")

import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

sentence2 = "القدس مدينة تاريخية تقع في "

def generate_text(text):
  try:
    input_ids = tokenizer.encode(sentence2, return_tensors='pt').to(device) 
    model.to(device)
    max_length = 100
    #set top_k = 40 and num_return_sequences = 3
    sample_outputs = model.generate(input_ids, pad_token_id=50256,
                                      do_sample=True, 
                                      max_length=max_length, 
                                      min_length=max_length,
                                      top_k=40,
                                      num_return_sequences=5)

    for i, sample_output in enumerate(sample_outputs):
        print(">> Generated text {}\n\n{}".format(i+1, tokenizer.decode(sample_output.tolist())))
        print('\n---')
  except exception as e:
    print("Something Wrong",e)

generate_text(sentence2)

"""GPT2-Small-Arabic-Poetry"""

tokenizer= AutoTokenizer.from_pretrained("akhooli/gpt2-small-arabic-poetry")
model = AutoModelWithLMHead.from_pretrained("akhooli/gpt2-small-arabic-poetry")

sentence3 = "الحب"

def generate_text(text):
  try:
    input_ids = tokenizer.encode(sentence3, return_tensors='pt').to(device) 
    model.to(device)
    max_length = 100
    #set top_k = 40 and num_return_sequences = 3
    sample_outputs = model.generate(input_ids, pad_token_id=50256,
                                      do_sample=True, 
                                      max_length=max_length, 
                                      min_length=max_length,
                                      top_k=40,
                                      num_return_sequences=5)

    for i, sample_output in enumerate(sample_outputs):
        print(">> Generated text {}\n\n{}".format(i+1, tokenizer.decode(sample_output.tolist())))
        print('\n---')
  except exception as e:
    print("Something Wrong",e)

generate_text(sentence3)
